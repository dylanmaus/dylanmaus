<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.1">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2024-09-13T01:03:03-04:00</updated><id>/feed.xml</id><title type="html">Dylan Maus</title><subtitle>Software Engineer</subtitle><entry><title type="html">Deep Learning Math</title><link href="/projects/deep+learning+math" rel="alternate" type="text/html" title="Deep Learning Math" /><published>2022-10-16T00:00:00-04:00</published><updated>2022-10-16T00:00:00-04:00</updated><id>/projects/Deep-Learning-Math</id><content type="html" xml:base="/projects/deep+learning+math"><![CDATA[<h1 id="introduction">Introduction</h1>

<p>Consider the function \(z(x, y) = a \sin(x) + b \cos(y)\) and noisy data from sensors that measure \(z\) at many \((x, y)\) locations. Our goal is to fit the data to \(z(x, y)\) and find the parameters \(a\) and \(b\). A nonlinear least squares approach such as the Gauss-Newton algorithm could be used to solve this nonlinear regression problem.</p>

<p>In a real world scenario we may not even know the function \(z\), so it must be approximated. To solve this more advanced problem, we propose a model i.e. an artificial neural network to approximate \(z\). The parameters of this model must be iteratively tuned to fit the data.</p>

<p>This post attempts to explain some of the math behind Deep Learning and why it may be preferred to other mathematical approaches.</p>

<h1 id="deep-learning">Deep Learning</h1>
<p>To approximate the unknown function \(z\), consider a 2 layer fully connected neural network with 2 inputs, 3 nodes in the hidden layer, 1 output, and sigmoid activation function.</p>

\[\mathbf{x} = \begin{bmatrix} 
x_0 \\
x_1
\end{bmatrix}\]

\[\mathbf{W}_0 = \begin{bmatrix} 
\omega_0&amp; \omega_1 \\
\omega_2&amp; \omega_3 \\
\omega_4&amp; \omega_5
\end{bmatrix}\]

\[\mathbf{b}_0 = \begin{bmatrix} 
b_0 \\
b_1 \\
b_2
\end{bmatrix}\]

\[\mathbf{W}_1 = \begin{bmatrix} 
\omega_6 \\
\omega_7 \\
\omega_8
\end{bmatrix}\]

\[\mathbf{b}_1 = \begin{bmatrix} 
b_3
\end{bmatrix}\]

\[F(\mathbf{x}) = \sigma(\sigma(\mathbf{x}\mathbf{W}_0 + \mathbf{b}_0)\mathbf{W}_1 + \mathbf{b}_1)\]

\[F(\mathbf{x}) = \sigma\left(\begin{bmatrix} 
\sigma(\omega_0 x_0 + \omega_1 x_1 + b_0) \\
\sigma(\omega_2 x_0 + \omega_3 x_1 + b_1) \\
\sigma(\omega_4 x_0 + \omega_5 x_1 + b_2) \\
\end{bmatrix}
\begin{bmatrix} 
\omega_6 \\
\omega_7 \\
\omega_8
\end{bmatrix}
+ b_3\right)\]

\[F(\mathbf{x}) = \sigma(
\sigma(\omega_0 x_0 + \omega_1 x_1 + b_0)\omega_6 +
\sigma(\omega_2 x_0 + \omega_3 x_1 + b_1)\omega_7 +
\sigma(\omega_4 x_0 + \omega_5 x_1 + b_2)\omega_8 +
b_3)\]

<p>Mean squared error</p>

\[L = \frac{1}{N} \sum_{i=0}^N (f(\mathbf{x}_i) - y_i)^2\]

<!-- | ![](/assets/plot.svg) | 
|:--:|
| $$ z(x, y) = a \sin(x) + b \cos(y) $$ |

<img src="/assets/plot.svg" alt="drawing" width="800"/> -->
<h1 id="data-fitting">Data Fitting</h1>

<p>The simplest example is fitting a linear function to data \(f(x; m, b) = y = mx + b\)</p>

<p>Testing \(\LaTeX\) \(\nabla_{\boldsymbol{x}} J(\boldsymbol{x})\)</p>]]></content><author><name></name></author><category term="projects" /><summary type="html"><![CDATA[Introduction]]></summary></entry></feed>